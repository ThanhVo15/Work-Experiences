{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Original Link Page Count\n",
      "0   https://www.classic.com/m/porsche/911/964/carr...          2\n",
      "1   https://www.classic.com/m/porsche/911/993/carr...         14\n",
      "2     https://www.classic.com/m/porsche/911/993/turbo         16\n",
      "3   https://www.classic.com/m/porsche/911/g-body/c...         20\n",
      "4   https://www.classic.com/m/porsche/911/g-body/9...         26\n",
      "5   https://www.classic.com/m/mercedes-benz/s/w126...         11\n",
      "6   https://www.classic.com/m/mercedes-benz/sl/r10...         76\n",
      "7     https://www.classic.com/m/mercedes-benz/sl/w121         24\n",
      "8   https://www.classic.com/m/mercedes-benz/g/w463...          7\n",
      "9   https://www.classic.com/m/bmw/3-series/e30/m3/...         14\n",
      "10        https://www.classic.com/m/bmw/z3/roadster/m         21\n",
      "11  https://www.classic.com/m/bmw/3-series/e36/m3/...         23\n",
      "12  https://www.classic.com/m/lancia/delta/1st-gen...          4\n",
      "13        https://www.classic.com/m/ferrari/458/coupe          8\n",
      "14  https://www.classic.com/m/ferrari/f355/spider-...         11\n",
      "15  https://www.classic.com/m/ford/mustang/1st-gen...         29\n",
      "16  https://www.classic.com/m/chevrolet/camaro/1st...         26\n",
      "17  https://www.classic.com/m/chevrolet/corvette/c...         19\n",
      "18              https://www.classic.com/m/nissan/gt-r         14\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "def crawl_page_count(driver, original_url):\n",
    "    try:\n",
    "        # Find the element containing the page count\n",
    "        page_count_element = driver.find_element(By.XPATH, \"(//*[@class= 'typography-subtitle1 typography-primary-color typography-sm- typography-md- typography-lg-'])[3]\")\n",
    "        page_count_text = page_count_element.text\n",
    "        page_count = int(page_count_text) if page_count_text.isdigit() else 0\n",
    "    except NoSuchElementException:\n",
    "        # If no page count is found, return 1\n",
    "        page_count = 1\n",
    "    \n",
    "    return page_count\n",
    "\n",
    "# Initialize the browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "file_path = r\"\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    arr_urls = file.read().splitlines()\n",
    "\n",
    "# Create DataFrame to store data\n",
    "df_links = pd.DataFrame(columns=['Original Link', 'Page Count'])\n",
    "\n",
    "for original_url in arr_urls:\n",
    "    # Access the original page\n",
    "    driver.get(original_url)\n",
    "    \n",
    "    # Get the page count for the current link\n",
    "    page_count = crawl_page_count(driver, original_url)\n",
    "    \n",
    "    # Append the page count to DataFrame\n",
    "    df_temp = pd.DataFrame({'Original Link': original_url, 'Page Count': page_count}, index=[0])\n",
    "    df_links = pd.concat([df_links, df_temp], ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "df_links.to_csv('page_counts.csv', index=False)\n",
    "\n",
    "# Print DataFrame to console for inspection\n",
    "print(df_links)\n",
    "\n",
    "# Close the browser when done\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Original Link  \\\n",
      "0    https://www.classic.com/m/porsche/911/964/carr...   \n",
      "1    https://www.classic.com/m/porsche/911/964/carr...   \n",
      "2    https://www.classic.com/m/porsche/911/993/carr...   \n",
      "3    https://www.classic.com/m/porsche/911/993/carr...   \n",
      "4    https://www.classic.com/m/porsche/911/993/carr...   \n",
      "..                                                 ...   \n",
      "360              https://www.classic.com/m/nissan/gt-r   \n",
      "361              https://www.classic.com/m/nissan/gt-r   \n",
      "362              https://www.classic.com/m/nissan/gt-r   \n",
      "363              https://www.classic.com/m/nissan/gt-r   \n",
      "364              https://www.classic.com/m/nissan/gt-r   \n",
      "\n",
      "                                             Page Link  \n",
      "0    https://www.classic.com/m/porsche/911/964/carr...  \n",
      "1    https://www.classic.com/m/porsche/911/964/carr...  \n",
      "2    https://www.classic.com/m/porsche/911/993/carr...  \n",
      "3    https://www.classic.com/m/porsche/911/993/carr...  \n",
      "4    https://www.classic.com/m/porsche/911/993/carr...  \n",
      "..                                                 ...  \n",
      "360      https://www.classic.com/m/nissan/gt-r?page=10  \n",
      "361      https://www.classic.com/m/nissan/gt-r?page=11  \n",
      "362      https://www.classic.com/m/nissan/gt-r?page=12  \n",
      "363      https://www.classic.com/m/nissan/gt-r?page=13  \n",
      "364      https://www.classic.com/m/nissan/gt-r?page=14  \n",
      "\n",
      "[365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def generate_page_links(original_url, page_count):\n",
    "    # Initialize an empty list to store the generated page links\n",
    "    page_links = []\n",
    "    \n",
    "    # Generate links for each page based on the page count\n",
    "    for page_number in range(1, page_count + 1):\n",
    "        page_url = f\"{original_url}?page={page_number}\"\n",
    "        page_links.append(page_url)\n",
    "    \n",
    "    return page_links\n",
    "\n",
    "# Read the page counts DataFrame from the CSV file\n",
    "page_count = pd.read_csv('page_counts.csv')\n",
    "df1 = page_count.copy()\n",
    "\n",
    "# Create an empty DataFrame to store generated page links\n",
    "df_page_links = pd.DataFrame(columns=['Original Link', 'Page Link'])\n",
    "\n",
    "# Iterate through each link and generate page links\n",
    "for index, row in df1.iterrows():\n",
    "    original_url = row['Original Link']\n",
    "    df1 = row['Page Count']\n",
    "    \n",
    "    # Generate page links for the current link\n",
    "    page_links = generate_page_links(original_url, df1)\n",
    "    \n",
    "    # Append page links to DataFrame\n",
    "    for page_link in page_links:\n",
    "        df_temp = pd.DataFrame({'Original Link': original_url, 'Page Link': page_link}, index=[0])\n",
    "        df_page_links = pd.concat([df_page_links, df_temp], ignore_index=True)\n",
    "\n",
    "# Save generated page links to a text file\n",
    "with open('page_links.txt', 'w') as file:\n",
    "    for page_link in df_page_links['Page Link']:\n",
    "        file.write(page_link + '\\n')\n",
    "\n",
    "# Print generated page links DataFrame for inspection\n",
    "print(df_page_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def crawl_link_per_page(driver, original_url, df_links, found_links_file):\n",
    "    # Lấy số lượng trang\n",
    "    page_count_element = driver.find_element(By.XPATH, \"(//*[@class= 'typography-subtitle1 typography-primary-color typography-sm- typography-md- typography-lg-'])[1]\")\n",
    "    page_count_text = page_count_element.text\n",
    "    page_count = int(page_count_text) if page_count_text.isdigit() else 0\n",
    "\n",
    "    # Lấy thời gian bắt đầu tạo\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Lấy các phần tử\n",
    "    df_links_temp = pd.DataFrame(columns=['Original Link', 'Page Count', 'Name', 'Found Link', 'Date', 'Time'])\n",
    "    elements = driver.find_elements(By.XPATH, \"//*[@class= 'text-xl leading-5 font-medium table:text-secondary table:text-base flex-1']\")\n",
    "    for element in elements:\n",
    "        try:\n",
    "            found_link = element.get_attribute(\"href\")\n",
    "            name = element.text\n",
    "            \n",
    "            # Lưu vào tệp và DataFrame\n",
    "            with open(found_links_file, 'a') as file:\n",
    "                file.write(found_link + '\\n')\n",
    "            df_temp = pd.DataFrame({'Original Link': original_url, 'Page Count': page_count, 'Name': name, 'Found Link': found_link, 'Date': current_time.split()[0], 'Time': current_time.split()[1]}, index=[0])\n",
    "            df_links_temp = pd.concat([df_links_temp, df_temp], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing element: {e}\")\n",
    "        \n",
    "    # Thêm vào DataFrame chính\n",
    "    df_links = pd.concat([df_links, df_links_temp], ignore_index=True)\n",
    "    \n",
    "    return df_links\n",
    "\n",
    "def collect_found_links(driver, df_links, found_links_file):\n",
    "    # Thu thập các liên kết đã tìm thấy và nối vào DataFrame df_links\n",
    "    with open(found_links_file, 'r') as file:\n",
    "        found_urls = file.read().splitlines()\n",
    "\n",
    "    for found_url in found_urls:\n",
    "        # Lấy thời gian bắt đầu collect\n",
    "        start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        driver.get(found_url)\n",
    "        time.sleep(5)  \n",
    "\n",
    "        try:\n",
    "            view_listing_element = driver.find_element(By.XPATH, \"//div[@id='status-banner-analytics']/a\")\n",
    "            view_listing_href = view_listing_element.get_attribute('href')\n",
    "\n",
    "            # Update the DataFrame with the link if found\n",
    "            df_links.loc[df_links['Found Link'] == found_url, 'Link View Listing'] = view_listing_href\n",
    "            # Thêm thời gian bắt đầu collect\n",
    "            df_links.loc[df_links['Found Link'] == found_url, 'Start Time'] = start_time\n",
    "        except NoSuchElementException as e:\n",
    "            print(f'Không tìm thấy liên kết xem danh sách cho {found_url}: {e}')\n",
    "    \n",
    "    return df_links\n",
    "\n",
    "# Khởi tạo trình duyệt\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "file_path = r\"D:\\GitHub\\Work-Experiences\\Tuan Loc Commodities\\2. Scrapping\\page_links.txt\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    arr_urls = file.read().splitlines()\n",
    "\n",
    "# Tạo DataFrame để lưu trữ dữ liệu\n",
    "df_links = pd.DataFrame(columns=['Original Link', 'Page Count', 'Name', 'Found Link', 'Date', 'Time', 'Start Time'])\n",
    "\n",
    "found_links_file = r\"D:\\GitHub\\Work-Experiences\\Tuan Loc Commodities\\2. Scrapping\\found links.txt\"\n",
    "\n",
    "for original_url in arr_urls:\n",
    "    # Truy cập trang gốc\n",
    "    driver.get(original_url)\n",
    "    # Lấy dữ liệu từ trang\n",
    "    df_links = crawl_link_per_page(driver, original_url, df_links, found_links_file) \n",
    "    time.sleep(5)\n",
    "\n",
    "# Thu thập các liên kết đã tìm thấy từ arr_urls và nối vào DataFrame df_links\n",
    "df_links = collect_found_links(driver, df_links, found_links_file)\n",
    "\n",
    "# Lưu DataFrame vào tệp CSV\n",
    "df_links.to_csv('df_links.csv', index=False)\n",
    "\n",
    "# In DataFrame ra console để kiểm tra\n",
    "print(df_links)\n",
    "\n",
    "# Đóng trình duyệt khi hoàn tất\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def crawl_link_per_page(driver, original_url, df_links, found_links_file):\n",
    "    # Lấy số lượng trang\n",
    "    page_count_element = driver.find_element(By.XPATH, \"(//*[@class= 'typography-subtitle1 typography-primary-color typography-sm- typography-md- typography-lg-'])[1]\")\n",
    "    page_count_text = page_count_element.text\n",
    "    page_count = int(page_count_text) if page_count_text.isdigit() else 0\n",
    "\n",
    "    # Lấy thời gian bắt đầu tạo\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Lấy các phần tử\n",
    "    df_links_temp = pd.DataFrame(columns=['Original Link', 'Page Count', 'Name', 'Found Link', 'Status', 'Date', 'Time'])\n",
    "    elements = driver.find_elements(By.XPATH, \"//*[@class= 'text-xl leading-5 font-medium table:text-secondary table:text-base flex-1']\")\n",
    "    for element in elements:\n",
    "        try:\n",
    "            found_link = element.get_attribute(\"href\")\n",
    "            name = element.text\n",
    "            \n",
    "            # Lưu vào tệp và DataFrame\n",
    "            with open(found_links_file, 'a') as file:\n",
    "                file.write(found_link + '\\n')\n",
    "            df_temp = pd.DataFrame({'Original Link': original_url, 'Page Count': page_count, 'Name': name, 'Found Link': found_link, 'Status': '', 'Date': current_time.split()[0], 'Time': current_time.split()[1]}, index=[0])\n",
    "            df_links_temp = pd.concat([df_links_temp, df_temp], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing element: {e}\")\n",
    "        \n",
    "    # Thêm vào DataFrame chính\n",
    "    df_links = pd.concat([df_links, df_links_temp], ignore_index=True)\n",
    "    \n",
    "    return df_links\n",
    "\n",
    "def collect_found_links(driver, df_links, found_links_file):\n",
    "    # Thu thập các liên kết đã tìm thấy và nối vào DataFrame df_links\n",
    "    with open(found_links_file, 'r') as file:\n",
    "        found_urls = file.read().splitlines()\n",
    "\n",
    "    for found_url in found_urls:\n",
    "        # Lấy thời gian bắt đầu collect\n",
    "        start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        driver.get(found_url)\n",
    "        time.sleep(3)  # Thêm một đợi nhỏ để ổn định, bạn có thể điều chỉnh nó\n",
    "        try:\n",
    "            # Try finding the view listing element using the first XPath expression\n",
    "            try:\n",
    "                view_listing_element = driver.find_element(By.XPATH, \"//*[@class= 'flex md:inline-flex items-center justify-center  uppercase font-medium tracking-wider whitespace-nowrap rounded transition duration-200 text-white bg-blue-500 hover:bg-blue-500/90 border border-blue-500 hover:border-blue-500/90 shadow-lg w-full h-full px-5 py-1']\")\n",
    "                status_element = driver.find_element(By.XPATH, \"//*[@class= 'border font-medium uppercase inline-block whitespace-nowrap text-white bg-black border-black px-2 text-lg rounded']\")\n",
    "            except NoSuchElementException:\n",
    "                # If the first XPath expression doesn't find the elements, try the second ones\n",
    "                view_listing_element = driver.find_element(By.XPATH, \"//*[@class= 'flex md:inline-flex items-center justify-center px-5 py-2 uppercase font-medium tracking-wider whitespace-nowrap rounded transition duration-200 text-blue-500 border border-blue-500 hover:bg-blue-50 w-full h-full']\")\n",
    "                try:    \n",
    "                    status_element = driver.find_element(By.XPATH, \"//*[@class= 'border font-medium uppercase inline-block whitespace-nowrap text-green-600 border-green-600 px-2 text-lg rounded']\")\n",
    "                except NoSuchElementException:\n",
    "                    status_element = driver.find_element(By.XPATH, \"//*[@class= 'border font-medium uppercase inline-block whitespace-nowrap text-red-600 border-red-600 px-2 text-lg rounded']\")\n",
    "\n",
    "            status = status_element.text\n",
    "\n",
    "            # If the element is found, get its href attribute\n",
    "            view_listing_href = view_listing_element.get_attribute('href')\n",
    "\n",
    "            # Update the DataFrame with the link if found\n",
    "            df_links.loc[df_links['Found Link'] == found_url, 'Status'] = status\n",
    "            df_links.loc[df_links['Found Link'] == found_url, 'Link View Listing'] = view_listing_href\n",
    "            # Thêm thời gian bắt đầu collect\n",
    "            df_links.loc[df_links['Found Link'] == found_url, 'Start Time'] = start_time\n",
    "        except NoSuchElementException as e:\n",
    "            print(f'Không tìm thấy liên kết xem danh sách cho {found_url}: {e}')\n",
    "    \n",
    "    return df_links\n",
    "\n",
    "# Khởi tạo trình duyệt\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "file_path = r\"D:\\GitHub\\Work-Experiences\\Tuan Loc Commodities\\2. Scrapping\\page_links copy.txt\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    arr_urls = file.read().splitlines()\n",
    "\n",
    "# Tạo DataFrame để lưu trữ dữ liệu\n",
    "df_links = pd.DataFrame(columns=['Original Link', 'Page Count', 'Name', 'Found Link', 'Status', 'Date', 'Time', 'Start Time'])\n",
    "\n",
    "found_links_file = r\"D:\\GitHub\\Work-Experiences\\Tuan Loc Commodities\\2. Scrapping\\found links.txt\"\n",
    "\n",
    "for original_url in arr_urls:\n",
    "    # Truy cập trang gốc\n",
    "    driver.get(original_url)\n",
    "    # Lấy dữ liệu từ trang\n",
    "    df_links = crawl_link_per_page(driver, original_url, df_links, found_links_file) \n",
    "    time.sleep(5)\n",
    "\n",
    "# Thu thập các liên kết đã tìm thấy từ arr_urls và nối vào DataFrame df_links\n",
    "df_links = collect_found_links(driver, df_links, found_links_file)\n",
    "\n",
    "# Lưu DataFrame vào tệp CSV\n",
    "df_links.to_csv('df_links.csv', index=False)\n",
    "\n",
    "# In DataFrame ra console để kiểm tra\n",
    "print(df_links)\n",
    "\n",
    "# Đóng trình duyệt khi hoàn tất\n",
    "driver.quit()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
